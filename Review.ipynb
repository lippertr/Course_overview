{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Write a one line list comprehension to perform the following matrix\n",
    "multiplication (A dot product B).  Describe how it works. This should be generalizable - do NOT hard code in the size of A and B. How would you check\n",
    "it with NumPy?\n",
    "```python\n",
    "    A = [ [ 2, 4], [ 1, 7], [-1, 8] ]\n",
    "    B = [ [3, 2, -5, 6],  [1, -3, 4, 8] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "```python\n",
    "    import numpy as np\n",
    "    def matrix_multiplication(A, B):\n",
    "        return [[sum(A[i][j]*B[j][k] for j in range(len(A[0])))\n",
    "            for k in range(len(B[0]))] for i in range(len(A))]\n",
    "    if __name__ == '__main__':\n",
    "        A = [ [ 2, 4], [ 1, 7], [-1, 8] ]\n",
    "        B = [ [3, 2, -5, 6],  [1, -3, 4, 8] ]\n",
    "        print(\"Matrix multiplication results:\")\n",
    "        mat = matrix_multiplication(A, B)\n",
    "        for line in mat:\n",
    "            print(line)\n",
    "        print(\"\\nNumpy results:\")\n",
    "        Anp = np.array(A)\n",
    "        Bnp = np.array(B)\n",
    "        print(np.dot(Anp,Bnp))\n",
    "        #[[ 10  -8   6  44]\n",
    "        # [ 10 -19  23  62]\n",
    "        # [  5 -26  37  58]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "SQL: Given table `houses` below, write a query to...\n",
    "| id | sqft | beds | neighborhood | type | sale_price |\n",
    "|:----------:|:------------:|:----------:|:----------:|:-----------:|:-----------:|\n",
    "| 1 | 1150 | 2 | prospect-park | townhome | 244052 |\n",
    "| 2 | 2600 | 3 | calhoun-isles | single_family | 609536 |\n",
    "| 3 | 860 | 1 | uptown | condo | 472993 |\n",
    "| 4 | 1320 | 3 | north-loop | townhome | 309485 |\n",
    "| 5 | 1030 | 2 | downtown | townhome | 456141 |\n",
    "| 6 | 3000 | 3 | uptown | single_family | 544431 |\n",
    "| 7 | 1400 | 2 | longfellow | condo | 305314 |\n",
    "| 8 | 3000 | 4 | longfellow | single_family | 485802 |\n",
    "| 9 | 1700 | 3 | stephens-square | single_family | 337029 |\n",
    "  * Return two statistics: the average price per bedroom, and the average price per square foot. (This should be a row with two columns - and be careful how you compute this)\n",
    "  * Return the neighborhood having the highest number of single family homes per sale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2\n",
    "\n",
    "  * **Return two statistics: the average price per bedroom, and the average price per square foot. (This should be a row with two columns - and be careful how you compute this)**\n",
    "  ```SQL\n",
    "  SELECT SUM(sale_price)/SUM(beds) AS avg_price_per_br,\n",
    "         SUM(sale_price)/SUM(sqft) AS avg_price_per_sqft\n",
    "  FROM houses\n",
    "  ```\n",
    "  * **Return the neighborhood having the highest number of single family homes per sale.**\n",
    "  ```SQL\n",
    "  SELECT neighborhood\n",
    "  FROM\n",
    "  (SELECT neighborhood, COUNT(*) as inventory  \n",
    "  FROM houses\n",
    "  WHERE type = 'single_family'\n",
    "  GROUP BY neighborhood) temp\n",
    "  GROUP BY neighborhood\n",
    "  HAVING inventory = MAX(inventory)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "What probability distribution would you use to describe the following situations?\n",
    "  * How many customers arrive at the Starbucks in a certain time window.\n",
    "  * Modeling the distribution of SAT scores (hint: treat them as continuous).\n",
    "  * The number of defective parts that come off of an assembly line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3\n",
    " * **How many customers arrive at the Starbucks in a certain time window.**\n",
    "    Poisson\n",
    "  * **Modeling the distribution of SAT scores (hint: treat them as continuous).**\n",
    "    Normal\n",
    "  * **The number of defective parts that come off of an assembly line.**\n",
    "    Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Your friend flips a coin, then rolls dice/die, and tells you the total on the die. If the coin shows heads, she rolls one die. If it shows tails, she rolls two dice. What is the probability that the coin came up heads, given that the die/dice total is 6.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4\n",
    "<br>P(H|dice=6) = P(dice=6|H)P(H) / P(dice = 6)\n",
    "<br>P(dice=6) = P(dice=6|H)P(H) + P(dice=6|T)P(T)\n",
    "<br>          = (1/6 x 1/2)     + (5/36 x 1/2)\n",
    "<br>          = 1/12 + 5/72\n",
    "<br>P(H|dice=6) = P(dice=6|H)P(H) / P(dice = 6)\n",
    "<br>            = 1/12  /   (1/12 + 5/72)\n",
    "<br>            = 0.54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "My company is comparing 4 different landing pages, to see which gets the best click through rate. We are a new company and so there is no baseline to compare with. The biggest difference in performance was page C over page A, with a p-value of .02. Is this significant? Page C is slightly more expensive to maintain. Should I implement Page C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 5\n",
    "Comparing the performance of 4 pages involves making 4C2 = 6 comparisons. At a desired significance level of p = 0.05 for the experiment, a single test yielding a p-value of 0.02 is NOT significant if you apply the Bonferroni correction. Since page C is more expensive to maintain, you do not have significant evidence that page C is worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    " Draw the distributions associated with a hypothesis test between two means.  Label the critical value, the Type I error, and Type II error.  Indicate on the diagram how you'd calculate the statistical power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "What is the central limit theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 7\n",
    "The central limit theorem (CLT) states that the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined (finite) expected value and finite variance, will be approximately normally distributed, regardless of the underlying distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "In the context of machine learning, what are bias and variance?  And what is the bias-variance trade-off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 8\n",
    "Bias is the offset of the expected value of a prediction from the true value.  Variance is the scatter of the predictions around the expected value.  The bias-variance tradeoff is the general inverse relationship between the two:  as model complexity (or flexibility) changes either bias will increase and variance will decrease, or vice versa.  This occurs because the underlying signal is unknown and the data that the model trains on has some irreducible error that is also unknown.  Therefore, changes in model complexity will change the ability of the model to fit the training data, and its performance on test data will be affected by the inherent assumption associated with the functional form of the \"signal\" in the model and its relation to the true underlying signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "What are the assumptions behind OLS linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 9\n",
    " Linear relationship between predictors and response.\n",
    "* Linear independence of regressors (X)\n",
    "* Errors are normally distributed\n",
    "* Errors have mean of 0 (“strict exogeneity”).  E[e | X] = 0\n",
    "* (Spherical errors)\n",
    "  * Homoscedasticity: constant variance of errors\n",
    "  * No autocorrelation of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Suppose you work for company that employs a model to predict fraud. The confusion matrix for this model looks like this, where P means a fraudulent transaction, and N means a non-fraudulent transaction.\n",
    "  || Predict P | Predict N |\n",
    "  |----------|:---------:|:---------:|\n",
    "  | Actual P |    111    |    105    |\n",
    "  | Actual N |    45     |    739    |   \n",
    "  Your employer says \"This model has 85% accuracy! I'm only interested in letting you work on a model if you come up with a model with better accuracy than this.\" What do you tell him/her? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 10\n",
    "I would tell him that accuracy is not always a good measure of success in a classification model, particularly one with class imbalance. There are nearly 4 times as many non-fraudulent transactions as there are fraudulent transactions, so this situation is definitely an example of class imbalance. If I knew (or could estimate) the costs of investigating fraud vs. missing fraud, I might show him a profit curve that showed the deficiencies of this model. Make the (very reasonable) assumption that missing fraud is substantially more expensive than investigating potential fraud, then the most important thing is to catch as many of the positive cases as we can, even at the expense of a few more false positives. This means we should prioritize the *recall* or *true positive rate* metric over accuracy. The existing model only has a recall of just over 50%, which could certainly be improved upon!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "Are gradient descent methods deterministic? Why or why not? What can you do to increase the probability of getting optimal results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 11\n",
    "They are not deterministic, as they are dependent on starting point, as well as learning rate and other hyperparameters. As such, they can converge to local rather than global minima.\n",
    "Ways that you can account for this include:\n",
    "using modified gradient descent methods such as adaptive learning rate models, or using momentum, and/or trying multiple starting points and choosing the result with the lowest cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "You are building a decision tree regressor and you split on a certain feature to get two child nodes with labels as follows:\n",
    "  A: [ 92,  77,  99, 105,  99,  91, 103, 110,  88, 103]\n",
    "  B: [101, 117, 117, 108, 106, 111, 103, 105, 121, 110]\n",
    "  What is the information gain achieved by this split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 12\n",
    "```python\n",
    "  A = [ 92,  77,  99, 105,  99,  91, 103, 110,  88, 103]\n",
    "  B = [101, 117, 117, 108, 106, 111, 103, 105, 121, 110]\n",
    "  parent = np.hstack([A, B])\n",
    "  IG = parent.var() - ((12/24)*np.var(A) + (12/24)*np.var(B))\n",
    "  IG\n",
    "  Out[5]: 43.56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "What is data leakage? What are all the steps you must take when designing a model and engineering an analysis pipeline to avoid this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 13\n",
    "Data Leakage is the creation of unexpected additional information in the training data, allowing a model or machine learning algorithm to make unrealistically good predictions. This will result in a model that appears to work well but generalizes poorly to real world-data.\n",
    "Steps than you should take to avoid this include:\n",
    "  * Making sure that your target column, or any column used to create the target column, do not appear in your features.\n",
    "  * Training all pre-processers (text processing, scaling, one-hot-encoders, etc.) only on training data, and using only the transform method on test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "Compare and contrast random forests and boosted trees with regards to:  \n",
    "  * The data that each tree in the ensemble is built on.  \n",
    "  * How the quality of a split on a given feature and its value is evaluated.  \n",
    "  * The general depth of each tree.\n",
    "  * The bias-variance trade-off associated with each tree in the ensemble.  \n",
    "  * How the ensemble can achieve a low-bias, low-variance model.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 14\n",
    " * **The data that each tree in the ensemble is built on.**  \n",
    "  Random forest: each tree is built on a bootstrapped sample of the training data.  \n",
    "  Boosting: after the first tree each tree is built on either re-weighted data (Adaboost) or on the residuals (gradient boosting) of the previous tree.\n",
    "  * **How is the quality of a split on a given feature and its value is evaluated.**  \n",
    "  In both cases the goal is still to maximize information gain: the maximum reduction in variance (regression) or entropy/gini impurity (classification)\n",
    "  * **The general depth of each tree.**  \n",
    "  Random forest: each tree is grown pretty deep.  \n",
    "  Boosting: usually the trees are decision stumps (only 1 or 2 splits)\n",
    "  * **The bias-variance trade-off associated with each tree in the ensemble.**  \n",
    "    Random forest:  being deep and potentially overfit, each tree will tend to have low bias but high variance.  \n",
    "    Boosting: being shallow and potentially underfit, each tree will tend to have high bias but low variance.\n",
    "  * **How the ensemble can achieve a low-bias, low-variance model.**  \n",
    "    Random forest: by taking the average or majority vote of many overfit learners that tend to have low bias and high variance, the result is an ensemble that has much lower variance but slightly higher bias than any individual tree in the ensemble.  \n",
    "    Boosting: by taking the end result of a number of weak rules of thumb, where each rule of thumb is a high bias, low variance model, the bias of the ensemble is greatly reduced while slightly increasing the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "Compare and contrast Adaboost and gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 15\n",
    "In both cases weak learners are built on the training result of the prior learner.  However, they differ in what the training data are.  In gradient boosting the training data are the residuals from the prior learner, while in Adaboost the training data are the original training data re-weighted by their residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16\n",
    "NLP:\n",
    "* What does tf-idf stand for? Why does this generally work better than other word-vectorization alternatives?\n",
    "* What does it mean to lemmatize a word? Why should you do this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 16\n",
    "* **What does tf-idf stand for? Why does this generally work better than other word-vectorization alternatives?**\n",
    "This stands for term-frequency, inverse-document-frequency. This works better than simple count-vectorization (term frequency) as it takes into account how often a word appears in a corpus. For example, if we were considering a corpus of book reviews, the word \"book\" would appear in nearly every document, but is not informative. Diving by the document frequency minimizes the impact of words that appear often - i.e. they describe the entire corpus but are not informative on a document level.\n",
    "* **What does it mean to lemmatize a word? Why should you do this?**\n",
    "To lemmatize a word, this means to take the word down to its root. This, in theory brings you down to the root concept of the word, so that you aren't treating present and past tenses, or single and plural versions, etc. of the same word differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 17\n",
    " Describe the \"contents\" of the matrices that come out of SVD decomposition, both their sizes and what they represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 17\n",
    "SVD returns three matrices, often called U, S, and V. The U matrix contains eigenvectors (basis vectors) that are linear combinations of the original features, orthogonal to the other rows. This matrix has dimensions n_rows, n_rows, with zeros padding the right outside. Non-zero size of n_rows, n_latent_features.\n",
    "The S matrix contains the singular values, or the square root of the eigenvalues associated with the eigenvectors represented in S. The square of the  singular value is the variance explained by this vector (can be normalized to represent the proportion of variance explained by this eigenvector).\n",
    "The V matrix has dimensions n_latent_features, n_features and contains eigenvectors that are linear combinations of the original rows.\n",
    "n_latent_features is a dimension that you can choose, and can be as large as the minimum of (n_features, n_rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 18\n",
    "Whiteboard the algorithm (psuedocode) for NMF using multiplicative updates to solve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 18\n",
    "```python\n",
    "n_features = k\n",
    "# V is matrix we are trying to decompose\n",
    "W = initialize_random_mat(V.shape[0], n_features)\n",
    "while not converged:\n",
    "  # fit H while holding W constant, keep values >= 0\n",
    "  H_new = least_sq_soln(W, V)\n",
    "  clip(H_new)\n",
    "  # fit W while holding H constant, keep values >= 0\n",
    "  W_new = least_sq_soln(H.T, V.T).T\n",
    "  clip(W_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 19\n",
    "What is the curse of dimensionality? What types of algorithms are affected by this? Give examples of supervised and unsupervised methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 19\n",
    "As the number of dimensions increase, the volume that the existing data occupy grows exponentially. You can relate the sample density in two differently dimensioned spaces through the relation N1^(1/D1) ~ N2^(1/D2) where N1 and N2 are the number of samples in dimensions D1 and D2.  In high dimensional spaces all distance based metrics are far apart. The algorithms that are affected by this all include a distance metric in part of their fitting/assumptions. Supervised methods include linear models including linear and logistic regression, K-nearest neighbors regressor and classifier, and support vector machines. In the unsupervised space, K-Means is the most obvious example but any other clustering algorithm using euclidean distance metrics will face the same issues.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 20\n",
    "Contrast collaborative vs. content based recommendation engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 20\n",
    "Collaborative recommendation engines are a supervised algorithm, and don't require the algorithm to \"know\" anything about the items, only how the users have rated them. Collaborative recommenders group users who have given similar ratings and then recommend new items to a user that those similar users have rated highly.\n",
    "Content based recommendation engines contain information about the item (text or genre information from an article or book, ingredients from a recipe, e.g.) and recommend items that are most similar based on some distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 21\n",
    "Whiteboard the algorithm for a breadth first search in graph analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 21\n",
    "```\n",
    "randomly choose starting vertex x\n",
    "mark x\n",
    "list L = x\n",
    "tree T = x\n",
    "while L nonempty\n",
    "choose some vertex v from front of list\n",
    "visit v\n",
    "for each unmarked neighbor w\n",
    "    mark w\n",
    "    add it to end of list\n",
    "    add edge vw to T\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 22\n",
    "Compare and contrast Hadoop MapReduce and Spark, and give some pros and cons of each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 22\n",
    "Spark is considered the successor to Hadoop MapReduce. Spark runs computations in memory, which is much faster than Hadoop MapReduce, which is disk-based. Spark is also more flexible - can do many operations on data that do not fall into the map-reduce paradigm. Spark is a new and rapidly developing tool, and thus it is buggy with a very unstable API, while MapReduce is mature and stable. If you have five year old MapReduce code, it will most likely still run, while five week old Spark code may already be deprecated!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 23\n",
    "Compare and contrast Spark RDDs and DataFrames. Which is faster, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 23\n",
    "DataFrames are build on top of RDDs (Resilient Distributed Datasets). DataFrames are newer and a higher level of abstraction. They provide a significant performance boost in python over RDDs, and performance is identical across platforms, unlike RDDs which perform better in scala than they do in python. This is because RDDs are performing computations in python and are using the MapReduce paradigm. DataFrames are all in Java under the hood, and are structured data with a schema (think SQL table) which is responsible for the consistent performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 24\n",
    "What does HDFS stand for? Why should you take special consideration when writing to and reading from this file system?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 24\n",
    "Hadoop distributed file system. Data stored in hdfs is written to multiple files distributed across partitions to allow parallelizable queries and processing. When writing data to this type of file system, data should be partitioned in an intelligent way so that the most common processes can eliminate certain partitions to read from, which greatly speeds up queries and run time. When reading from this file system, you should be aware of how the data is partitioned, so that you can take advantage of this efficiency for SQL queries, and can re-partition the data if it makes more sense for your further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 25\n",
    "When writing software, you will often need to optimize for speed, memory, or both. Name some of the data structures in python that allow faster computation, and some of the things you can use when memory is an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 25\n",
    "For speed: For faster lookup, sets and dictionaries use a hashing algorithm, so lookup is O(1) instead of O(N) like checking list membership. For linear algebra type operations, numpy arrays are faster than python lists, as numpy is written in C and operations are vectorized.\n",
    "For memory: iterators and generators are designed to only return elements from an iterable one at a time, so that you don't need to store a large list/dictionary/etc. in memory. Some examples of this are range(), .keys(), .values(), .items() on a dictionary, and functions available from the itertools library such as combinations. This is by no means an exhaustive list, you may have many different examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
