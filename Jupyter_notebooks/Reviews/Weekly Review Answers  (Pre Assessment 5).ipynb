{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 SQL/NOSQL\n",
    " **Compare and contrast SQL and NoSQL databases.**\n",
    "    SQL (structured query language) and NoSQL are similar in that they store information.  However, SQL databases are relational, meaning that there are relationships between entities in separate tables, while in NoSQL data of multiple types are stored in documents that are not related.  As SQL databases are more structured they require a schema - a defined relation between tables and columns in tables - before data can be stored in them, while in NoSQL databases this is not the case.\n",
    "    * **SQL**\n",
    "        * each column is of a certain data type (integer, string, date)\n",
    "        * each row is an entry in the table (an observation) that holds values for each one of the columns\n",
    "        * tables are specified by a schema that defines the structure of the data\n",
    "        * we specify the table structure ahead of time \n",
    "    * ** RDBMS Tradeoffs **\n",
    "        * Relational database management system\n",
    "        * pro, Pre-defined schema permits efficient, complex queries\n",
    "        * pro, Scales vertically so making queries and joins can be done quickly\n",
    "        * con, Difficult to restructure existing schema\n",
    "        * con, Often requires that all data live on the same server\n",
    "        * con, Doesn’t shard (horizontal partitioning across nodes) easily\n",
    "    * **NOSQL**\n",
    "       * Document-based storage system for semi-structured data, can have data redundancies\n",
    "        * Documents represented as JSON-like objects\n",
    "        * A change to database generally results in needing to change many documents\n",
    "        * Since there is redundancy, simple queries are often faster, but more complex queries are slower\n",
    "        * No schema, joins or transactions\n",
    "        * Doesn’t need to know structure of data in advance\n",
    "        * Auto-sharding, easily replicated across servers\n",
    "        * Cursor: when you ask MongoDB for data, it returns a pointer to the result set called a cursor\n",
    "        * Actual execution is delayed until necessary\n",
    "        **MongoDB: Documents**\n",
    "            * Documents are composed of fields as name:value pairs\n",
    "                * Field Names\n",
    "                    * must be strings\n",
    "                    * can't start with a dollar sign character\n",
    "                    * can't contain a dot character\n",
    "                    * can't contain the null character\n",
    "                    * _id is reserved for use as primary key\n",
    "                * Field Values\n",
    "                    * Can be a mixture of other documents, arrays, and arrays, of documents\n",
    "                * Queries\n",
    "                    * db.users.find()\n",
    "                    * db.users.find({age:33}).sort({name: 1})\n",
    "                    * db.users.find({age: {$gt: 30}}).count()\n",
    "                * Aggregations \n",
    "                    * db.users.aggregate([\n",
    "                    {$match: {country: ‘US’}},\n",
    "                    {$group: {‘_id’: ‘$age’,])\n",
    "\n",
    "\t* **Give examples data ideal for each type of database.**\n",
    "\t  SQL - structured, \"clean\", unchanging  \n",
    "\t  NoSQL - unstructured, changing with time\n",
    "\t* **What are the analogues to tables, rows, and columns in MongoDB?**\n",
    "\t  Collections, documents, and fields.\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 Coding Vanilla Neual Nets\n",
    "**You walk into a job interview and the interviewer takes a look are your resume and says \"Neural networks, great!  Tell me, how would you code a vanilla neural network using no libraries?  Feel free to use the white board.\"  What do you write and draw?**\n",
    "\n",
    "\tI'd start with a drawing:  \n",
    "\tx&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;h&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y   \n",
    "\to ----- o  \n",
    "\t&nbsp;&nbsp;&nbsp;\\&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;\\   \n",
    "\t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;o  \n",
    "\t&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;&nbsp;\\&nbsp;&nbsp;&nbsp;/   \n",
    "\to ----- o  \n",
    "\t*This drawing shows an input layer **x**, a hidden layer **h**, and an output layer **y**.  The circles (except for **x**) are neurons - computational units that take an input and typically transform it non-linearly into an output using an activation function.  All the circles in each layer are connected to all the circles in adjacent layers with weights.  The weights between **x** and **h** we'll call **Wxh**, and the weights betwen **h** and **y** we'll call **Why**.   For a neural network to be a useful model in supervised learning, it needs to be trained on data.  In this process, the weights of the network change.  The model's architecture, each layer's activation function, and the trained weights comprise a trained neural network.*   \n",
    "\t*Here's how the model would be trained (in pseudocode):*  \n",
    "\t```python\n",
    "\tfor epoch in epochs:\n",
    "\t\t # feed forward\n",
    "\t\t find the outputs of h using activation_h(x.dot.Wxh)\n",
    "\t\t find the output yp using activation_y(h.dot.Why)\n",
    "\n",
    "\t\t # calculate the loss\n",
    "\t\t yp_error = y - yp\n",
    "\n",
    "\t\t # back-propagate to find the gradients, grad_Why and grad_Wxh\n",
    "\n",
    "\t\t # Use gradient descent to update weights\n",
    "\t\t Why += alpha * grad_Why\n",
    "\t\t Wxh += alpha * grad_Wxh\n",
    "\t```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Net Notes**\n",
    "    * **Basic Gradient Descent Algorithm**\n",
    "        * Minimizing the cost function\n",
    "            * Look at the derivative\n",
    "            * go downwards towards where the derivative is zero\n",
    "            * Up date\n",
    "            * Regularize by learning rate\n",
    "    * Have data, throw some weights at it, put it through a nonlinear activation function\n",
    "    * One layer is very weak\n",
    "        * limited to being linear classifiers\n",
    "    * Perceptrons\n",
    "        * artificial neuron\n",
    "    * Back Propagation \n",
    "    * Speech recognition\n",
    "    * image net competition\n",
    "        CNN\n",
    "    * Take up much smaller storage space, more computationally demanding, takes awhile to train. Basically just storing weights\n",
    "    * Very easy to ensemble\n",
    "**Multilayer perceptron**\n",
    "        * MLP\n",
    "         * Computation unit: perceptron(neuron)\n",
    "            * input, weights, summation, activation function, prediction\n",
    "        * use a signle neuron to explain\n",
    "            * feed forward\n",
    "            * back propagation (get gradients)\n",
    "            * gradient descent and its flavors\n",
    "        * Gradient descent solution optiizers\n",
    "        * Multi-layer network\n",
    "**Feed Forward**\n",
    "    * calculate outputs from inputs (prediction)\n",
    "**Back Propagation**\n",
    "    * during training, change weights so that predicted output is closer to train output.\n",
    "**Potential Problems**\n",
    "    * instability, difficult visualizing the traininf process, interpretability\n",
    "    * The vanishing (or exploding) gradient problem during backpropagation\n",
    "**Autoencoders**\n",
    "    * form of unsupervised deep learning\n",
    "    * can be used with stndard row and column data, images, sequences\n",
    "    * model is trained to reconstruct the input as output\n",
    "    * common first step in deep lerning problems, helps generalize outputs\n",
    "    * Traditional - output is trained to reconstruct input\n",
    "    * Denoising - inputs have some noise added, output trained to reconstruct without the noise\n",
    "    * Sequence to Sequence - same as traditional, but with sequences\n",
    "    * Variational - encoder has additional constraint: must generate latent vectors that roughly follow a unit gaussian distribution. Can use this feature to feed new vectors into teh decoder and generate new \n",
    "    * helps get rid of noise\n",
    "    * helps you to learn what the compressed version looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 HyperParameters in Net \n",
    " **Describe the hyperparameters you can tune in a neural network.**\n",
    "\n",
    "\t* learning rate\n",
    "\t* optimization algorithm\n",
    "\t* batch size\n",
    "\t* network architecture: number of hidden layers, number of nodes per layer\n",
    "\t* weight initializations\n",
    "\t* regularization: dropout, batch normalization, L1/L2 penalty\n",
    "    * Most are particular to your application: read the literature and start with something that has been shown to work well.\n",
    "    * Architecture: the number of hidden layers, the number of nodes in each layer \n",
    "    * Activation functions\n",
    "    * Weight and bias initialization (for weights Karpathy recommends Xavier init.) \n",
    "    * Training method: Loss function, learning rate, batch size, number of epochs\n",
    "        * Learning rates are important and should be fiddled with\n",
    "    * Regularization: weight decay (loss function L1 and L2), dropout\n",
    "        * dropout, fraction of the weights that are not updates, helps it to perform well and helps to prevent overfitting. Only happens during training. \n",
    "    * Random seed\n",
    "        * helps to repreoduce results\n",
    "            * can evalutate how well tweaking parameters affects the result\n",
    "        * this should work, occasionally you'll get weird results, non-invertable matrices, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 Updating weights\n",
    " **Describe the process by which weights are updated in neural networks.  Please include in your answer a differentiation between batch, mini-batch, stochastic updating of the weights.**\n",
    "\tDuring training, weights in a neural network are updated using gradient descent.  To do this, the gradient has to be determined from back propagation.  Back-propagation is the recursive application of the calculus's chain rule back through the network to find how the loss varies with each weight in the network.  \n",
    "\tDuring training, the decision must be made how often to update the weights.  If the weights are updated after each datapoint, this is called stochastic gradient descent.  If the weights are updated after seeing all the data, it's called batch.  If the data are sent through in batches, where each batch is a subset of all the data, that's called mini-batch.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 CNN\n",
    " **You have a square image, 11 x 11 pixels.  A square convolutional filter moved over it, sized 3 x 3, with a stride of 1.**\n",
    "  * **How big is the resulting feature map?**\n",
    "\t\t9\n",
    "  * **If the stride were 2, how big is the resulting feature map?**\n",
    "      * Stride allows one to subsample the data\n",
    "\t\t5\n",
    "  * **If we'd like to keep the feature map the same size as the original image, what do you need to do?**\n",
    "      * layers with stride of 1, filters of size FxF and zer-padding with (F-1)/2 (will preserve size spatially)\n",
    "      * Pad the edges with zeros.\n",
    "  * **Extra credit!  For square images and filters, if n x n is the size of the image, f x f is the size of the filter, and s is the stride, what is the general relationship of these values to the size of the resulting feature map m x m assuming no padding.**\n",
    "\t\tm = (n - f + s) / s\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN**\n",
    "     * Accepts a volume of size W_1 x H_1 x D_1\n",
    "     * Requires four hyperparameters:\n",
    "         * Number of filters(kernels) K\n",
    "         * their spatial extent F\n",
    "         * the stride S\n",
    "         * the amount of zero padding P\n",
    "         * Can also have\n",
    "             * Pool\n",
    "                 * max, min, avg\n",
    "                 * reduces the size of your resultant image\n",
    "                 * Max pool 2x2\n",
    "                     * 32x32 goes to 16x16\n",
    "             \n",
    "     * Produces a volume of size W_2 x H_2 x D_2\n",
    "         * W_2 = (W_1 - F + 2P)/ S + 1\n",
    "         * H_2 = (H_1 - F + 2P)/S + 1 (ie width and height are computed equally by symmetry)\n",
    "         * D_2 = K\n",
    "      * With parameter sharing, it introduces $ F \\cdot F \\cdot D_1 $ weights per filter, for a total of $ (F \\cdot F \\cdot D_1) \\cdot $ K weights\n",
    "      * In the output volume, the $ \\mathbf{d}-th $\n",
    " **Why do CNNs work well with images?**    \n",
    "    * preserves the spatial relations of the pixels\n",
    "    * (in vanilla neural network you're treating things as independent columns)\n",
    "**Dimensions of input to vanilla neural newtork**\n",
    "    * nrows, ncols\n",
    "        *(none, ncols)\n",
    "**Dimensions of input to convulutional neural newtorks**\n",
    "    * (none(rows),image height, image weight , channels(RGB)(3 for color, 1 for b&w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RNN**\n",
    "* models based on the connection of simple computational units, loosely analagous to neurons in the human brain\n",
    "* difference is that you're taking a time series dimension into account. \n",
    "* it can model temporal, sequential behavior\n",
    "* used for\n",
    "    * captioning images\n",
    "    * handwriting\n",
    "    * speech recognition\n",
    "    * generating text\n",
    "    * stock price prediction\n",
    "    * news stories\n",
    "**Multi-layer perceptron**\n",
    "    * Maps input data to corresponding outputs.  \n",
    "    * Calculation feeds forwardthrough the network. \n",
    "    * Nodes are arranged in layers.\n",
    "    * Nodes have values for any input given the sum of inputs to the node and an activation function that transforms the sum to a non-linear output.\n",
    "    * The “learning” in the network is held by the trained values of the connections (the weights).  \n",
    "    * These weights start with random values.\n",
    "    * After feed forward yields a predicted output (yp), its difference (loss) is calculated from the real output (y). \n",
    "    * Back propagation estimates how much the loss varies due to each weight in the network (the gradient).\n",
    "    * Gradient descent uses the gradient and learning rate to tweak each of the weights so that the network predicts a little better next time.\n",
    "    * When the total loss reaches an acceptable level, stop.  Now it’s trained and ready to predict\n",
    "\n",
    "**Benefit of the intra-layer recurrent connections**\n",
    "    * The previous state of a node in a recurrent hidden layer (H_prev for coding purposes) can affect the value of itself or other nodes in the layer in the present time (it’s a directed cycle).\n",
    "    * This gives the net the ability to model sequential data.\n",
    "    * Feedforward and backpropagation work the same way.\n",
    "    * Learn Whh like all the other weights.  In a trained model all the weights are fixed.  It’s the activations of the nodes that changes with changes in sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 preprocessing data\n",
    "**In processing text to perform analyses, what steps are usually involved? When might you use, or not, some of these steps?**\n",
    "\n",
    "\t* Lowercase your text\n",
    "\t* Strip out miscellaneous spacing & punctuation  \n",
    "\t* Remove stop words\n",
    "\t* Stem or lemmatize the text\n",
    "\t* Use part-of-speech tagging  \n",
    "\t* Expand feature matrix with N-grams\n",
    "\n",
    "\tFor each step above, you need to consider for your use case if it makes sense.  For example, Lowercasing your text may not make sense if in your corpus STOP has a different meaning than stop.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 PCA/SVD\n",
    "**What is the difference between PCA and SVD?**\n",
    "\tPrincipal Component Analysis (PCA) and Singular Value Decomposition (SVD)   \n",
    "\tare two eigenvalue methods used to reduce a high-dimensional dataset into  \n",
    "\tfewer dimensions while retaining important information.  \n",
    "\n",
    "\tPerforming PCA on a matrix results in eigenvectors and eigenvalues for  \n",
    "\tthe matrix, while SVD results in U, Sigma, and V matrices where the singular  \n",
    "\tvalues present in the diagonal of the Sigma matrix are the square root of  \n",
    "\tthe eigenvalues.  \n",
    "   PCA same as SVD when the $\\Lambda$ is the same as $\\Sigma^2$\n",
    " **Why reduce data?**\n",
    "      * Curse of dimensionality\n",
    "            * points are 'far away' and its easy to voer fit small datasets\n",
    "    * Visualization\n",
    "    * Latent features\n",
    "        * often (especially with image/video date) the most relevant features are not explicitly present in the high dimension (raw) data\n",
    "    * remove correlation\n",
    "        * whith many many features there is often correlation\n",
    "**PCA**\n",
    "    * First goal is to remove correlation between features\n",
    "    * A side effect is that we can reduce the dimensionality of the data while perserving most of the variance in the data \n",
    "    * standardize or your eiganvalues will be skewed and you won't have the accurate representation of the variance\n",
    "   \n",
    "###### Use when\n",
    "    * kNN on high dimensional data\n",
    "    * clustering high dimensional data\n",
    "    * visualization\n",
    "    * extracting latent features\n",
    "\n",
    "###### Don't use when\n",
    "    * You need to retain interpretability of your feature space\n",
    "    * Your model doesn't need reduced dimensional data \n",
    "**SVD**\n",
    "    * Singular value decomposition\n",
    "    * if p is very large, calculating the covariance matric is very expensive, and finding the eigenvectors of that covariance matric is even more expensive\n",
    "    *Every matrix has a unique decomposition in the form \n",
    "$$\n",
    "M = U \\Sigma V^T\n",
    "$$\n",
    "    * M is an matrix\n",
    "    * U is column orthogonal $ U^TU = I $\n",
    "    * $\\Sigma$ is a diagonal matrix of positive values in decreasing order\n",
    "    * V is column orthogonal\n",
    "###### Execution\n",
    "* The idea goes like this: Say our dataset maps from space X to space Y. E.g. X might be “user space”, and Y might be “movie space”.\n",
    "* After doing SVD, we can interpret the SVD as mapping from X space to a “concept space”, then mapping from that “concept space” to Y space.\n",
    "* We then interpret this “concept space” as latent features of our dataset. We also sometimes call this space the topic space.\n",
    "\n",
    "$$\n",
    "M_{\\mathbf{n \\times p}} = U_{\\mathbf{n \\times k}}\\Sigma_{\\mathbf{k \\times k}}V^T_{\\mathbf{k \\times p}}\n",
    "$$\n",
    "U will be the user-to-topic matrix and V the movie-to-topic matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 KMeans Algorithm\n",
    "**Whiteboard the algorithm (psuedocode) for the KMeans algorithm.**\n",
    "\t```\n",
    "\t* initialize k centroid locations\n",
    "\t   - you may do this by randomly assigning each point to a cluster and  \n",
    "\t     then find the centroid of each cluster or use kmeans++ to pick  \n",
    "\t     starting cluster points far away from each other\n",
    "\t* then repeat until \"convergence\":  \n",
    "\t    * assign each data point to the nearest centroid  \n",
    "\t    * compute new centroids  \n",
    "\n",
    "\tWhere \"convergence\" is ideally where the points stop changing their cluster,  \n",
    "\tthough there may always be a few that oscillate back and forth as the  \n",
    "\tcentroid locations changes slightly.\n",
    "\t```\n",
    "###### **K Means**\n",
    "    * basic clustering algorithm\n",
    "    * unsupervised learning\n",
    "    * trying to group data, find underlying structures\n",
    "        * with out knowing tht classes exist orignally\n",
    "    * there aren't stark error metrics to compare models\n",
    "    * probably not deterministic\n",
    "    * must standardize data, usings distance metrics\n",
    "**How to initialize centroids?**\n",
    "    * Random Centroids/choice, (simplest) randomly choose k points from your data and make those your initial centroids\n",
    "    * Random Assignment, randomly assign each data point to a number 1-k and initialize the kth centroid to the average of the points with the kth label. ( probably the longest number of iterations if you start here) \n",
    "    * k-means++ chooses well spread initial centroids. First centroid is chosen at random, with subsequent centroids chosen with porbability proportional to the squared distance to the closest existing centroid. this is the default initialization in sklearn. \n",
    "        * goal is to have them evenly distributed\n",
    "\n",
    "**Stopping Criteria**\n",
    "    * a specified number of iterations (sklearn default : max_iter=1000)\n",
    "    * until the centroids don't change at all ( reasonable expectation in a small data set)\n",
    "    * until the centroid don't move by very much (skleanr default: tol = .0001\n",
    "\n",
    "\n",
    "**Choosing K**\n",
    "    * possible methods\n",
    "        * elbow plot, kind of a corner and stops improving too much after that point\n",
    "            * k where improvement diminishes\n",
    "            * values that drastically reduces the residual sum of squares with the clusters\n",
    "        * silhouette score\n",
    "            * check the mean Intra-Cluster Distance (variance within cluster) of one cluster and how do these compare to the distance of the centroid in the next nearest cluster.\n",
    "            * mean intra-cluster distance (a) and the mean nearest-cluster distance (b)\n",
    "            * (b-a)/max(a,b)\n",
    "            * values range from -1 to 1, with one being optimal and -1 being the worst. \n",
    "        * other methods (e.g. GAP statistic)\n",
    "        * domain knowledge\n",
    "        \n",
    "**K-Means Assumptions**\n",
    "    * picked the 'correct' k\n",
    "    * clusters have equal variance\n",
    "    * clusters are isotropic (variance spherical)(standardize our data)\n",
    "    * clusters do not have to conatin the same number of observations\n",
    "        * this is not looking for equal sized clusters\n",
    "    * this is not deterministic\n",
    "        * will find many local minimum\n",
    "            * start many times and find the best\n",
    "    * Susceptible to curse of dimensionality \n",
    "    * one hot encoded categorical can overwhelm - look into k-modes\n",
    "        * remember, using distance metrics\n",
    "    * try MiniBatchKMeans for large datasets ( finds local minima, so be careful)\n",
    "        * K means ++, pairwise distances gets computationally expensive with time.\n",
    "\n",
    "**DBScan Algorithm**\n",
    "    * Also computes clusters using distance metric, but decides the number of clusters for you\n",
    "    * With K-Means, you choose k - this is your main hyper -parameter\n",
    "    * With DBScan, your main hyper-parameter is eps,which is the maximum distance between two points that are allowed to be in the same ‘neighborhood’\n",
    "\n",
    "**Real world use cases**\n",
    "    * customer segmentation\n",
    "    * product segmentation\n",
    "    * image degmentation\n",
    "    * anomaly detection\n",
    "    * social network analysis\n",
    "    * this is often a first pass cluster algorithm, can get more sophisticated\n",
    "\n",
    "###### **Hierarchical Clustering**\n",
    "    * Type of 'agglomerative clustering' - we iteratively group observations together based on thier distance from one another\n",
    "    * as continue to group observations together we form a hierarchy of their similarities with one another \n",
    "    * this will answer differennt questions than KMeans - we no longer have to choose the number of clusters up front, instead we will have to define the nature of successive groups of observations (linkages)\n",
    "    * Results don't depend on intialization\n",
    "    * Not limited to euclidean distance as the similarity metric\n",
    "        * cosine similarity    \n",
    "           * dot product of the vectors, divided by norm, subtracted from 1\n",
    "    * easy visualization through dendrograms\n",
    "        * 'height of fusion' on dendrogram quantifies the seperation of clusters \n",
    "**Hierachical Clustering Algorithm**\n",
    "    * begin with n observations and a measure of dissimilarity (Euclidean dist, Cosine simalirty) of all pairs of points, treating each observation as it's own cluster\n",
    "    * Fuse the two 'clusters' that are most similar. The similarit of these two indicates the height on the dendogram where the fusion should be recorded \n",
    "    * compute the new pairwise similarites between the remaining clusters, rinse and repeat \n",
    "**Measures of (dis)simlarity between groups**\n",
    "    **single linkage**\n",
    "        *closest two points in the cluster and measure that distance\n",
    "        * the distance between two clusters is defined as the shortest distance between two points in each cluster \n",
    "        * Chaining - several clusters may be joined to just because of a few close cases \n",
    "    **Complete linkage**\n",
    "        * taking the furthest two points in every cluster and measuring that distance\n",
    "        *the distance between two clusters is defined as the furthest  distance between two points in each cluster \n",
    "        * Cluster outliers prevent otherwise close clusters from merging.\n",
    "    **Average linkage**\n",
    "        * take every point and compare it's distance to every other point between clusters. the ditance between two clusters in defined as the average distance between each point in one clusters to every point in the other cluster\n",
    "        * computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 Distance Metrics\n",
    " **Discuss the distance metrics used in clustering and their use cases.**\n",
    "\tThere are many distance metrics, but the euclidean distance and cosine  \n",
    "\tdistance are commonly used in clustering.  Euclidean distance is the   \n",
    "\t\"straight line\" distance between two points in space, computed as the  \n",
    "\tsquare root of the sum of the squared component differences between the  \n",
    "\ttwo points.  Cosine distance quantifies the degree to which two vectors  \n",
    "\tpoint in the same direction and is computed as 1 minus the cosine  \n",
    "\tsimilarity, so a cosine distance of 0 means the vectors are pointing in\n",
    "\tthe same direction, 1 indicate they are orthogonal, and 2 indicates they  \n",
    "\tare pointing in opposite directions.\n",
    "\n",
    "\tIn general, euclidean distance will be more useful in lower dimensional spaces, because with increasing dimensions everything tends to get farther away.  Cosine distance similarly also suffers from this curse, but as it only investigates if two vectors are pointing in the same direction it's easier for two vectors to be near.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 Clustering Success\n",
    " **Describe metrics used to measure the \"success\" of clustering and how you might use them to choose k in KMeans.**\n",
    "\n",
    "\t*SSE or WCV* - Within cluster \"variance\" or sum of squared errors - these will continue to decrease as you increase k, until k = n when they should be 0. With these measures you are looking for an \"elbow\" of the score vs. number of clusters, where adding additional clusters gives you diminishing returns. SSE is also used within the K-Means model to compare initializations across a single value of k. \n",
    "\t*Silhouette score* - compares the distance of a point to it's cluster center compared to the center of the nearest cluster, with 1 being the best score and -1 being the worst score. Silhouette scores can be aggregated for all of the points in a dataset, and then compared for all values of k you are considering. The highest aggregate Silhouette score will likely be your best choice for k.\n",
    "\n",
    "**Evaluating K-Means**\n",
    "    * How do we measure how well our clustering does?\n",
    "       * a good measure should quantify how similar things are within a cluster\n",
    "       * Want to minimise Intra-Cluster Variance or Within Cluster Variance (WCV)\n",
    "       * Where WCV for the kth cluster, 1/K times sum of x actual - x mean squared\n",
    "       *If we pick too many ks the variance will go to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11 MapReduce\n",
    " **How would you explain MapReduce to a 5th grader?**\n",
    "\tThe idea behind MapReduce is to send a computation to distributed data,\n",
    "\tperform the computations in parallel, and send the results back to a central  \n",
    "\tplace to combine them.  \n",
    "\n",
    "\tBlue, red, and green eggs are hidden in a back yard for an easter egg hunt.  \n",
    "\tAn adult asks a group of kids to figure out how many red, green, and blue eggs  \n",
    "\tare hidden.   All the kids go looking for the eggs (MAP), each kid figures out  \n",
    "\thow many red, blue, and green he or she found (local COMBINER), and then  \n",
    "\tthey bring their counts of the eggs together to find the total count of eggs  \n",
    "\tof each color (REDUCE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12 Profit Curve\n",
    " **Explain how a profit curve is made.**\n",
    "\tFirst a classification model makes predictive probabilities.  Classification  \n",
    "\tthresholds are derived from the sorted probabilities, and then a confusion  \n",
    "\tmatrix is made for each threshold.  All the values in each confusion matrix  \n",
    "\tare normalized into rates.  Then each confusion matrix is multiplied by a  \n",
    "\tcost benefit matrix to determine the profit (or cost) for that threshold.  \n",
    "\tWhen this profit is plotted for each threshold, starting with the most strict  \n",
    "\tthreshold (1) and then in descending order, a profit curve results.  The   \n",
    "\tprofit curve illustrates the correct threshold to pick as the one that  \n",
    "\tmaximizes the profit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13 Class Imbalance\n",
    "\n",
    " **You have a dataset whose labels are about 25% minority class.  What options are available to you to help you make a model that accommodates this imbalance?**\n",
    "\n",
    "\tThere are some practical steps you can take:\n",
    "\t\t* Stratifying the train_test_split\n",
    "\t\t* Some sklearn models (e.g. decision trees) allow you to increase the weight\n",
    "\t\t\tof the minority class in the cost function.\n",
    "\n",
    "\tYou can also use external information (such as a cost-benefit matrix) to  \n",
    "\tdetermine what threshold to use in a classification problem (a.k.a. profit  \n",
    "\tcurves.)\n",
    "\n",
    "\tYou can also try to change the imbalance by undersampling the majority class,   \n",
    "\toversampling the minority class (in the form of exact copies), or using  \n",
    "\tsomething like SMOTE to make more artificial instances of the minority class.  \n",
    "\n",
    "\tThe efficacy of whatever method you choose should be evaluated on a hold-out   \n",
    "\tset.  It may be that the best thing to do is nothing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
