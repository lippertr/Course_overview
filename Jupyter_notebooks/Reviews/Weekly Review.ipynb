{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gradient Descent: What is an objective function? Give an example of a cost function to minimize and a cost function to maximize.\n",
    "\n",
    "    \n",
    "\n",
    "    Gradient desent allows us to find the coeffiecients that miniminze a certain cost function. The gradient descent algorithm starts with an initial guess for the coefieients then repeatedly updates the parameters that reduce the cost function. This happens by finding the gradient and taking partial derivatives that drive each coeffcient in the direction of steepest descent\n",
    "    \n",
    "    Root mean square error is one to minimzie and loglikelyhood is a function to maximize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gradient Descent\n",
    "\n",
    "X = np.array([[-9, -7, -1],\n",
    "                [ 6, -5, -2],\n",
    "                [-7, -1,  8],\n",
    "                [ 3, -1,  6]])\n",
    "\n",
    "y = np.array([[-26],\n",
    "                [-10],\n",
    "                [ 15],\n",
    "                [ 19]])\n",
    "\n",
    "beta = np.array([[1],[1],[1]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_grad(X, y, beta, lr = .0001, max_iter = 10000):\n",
    "    for num in range(max_iter):\n",
    "        grad = -X.T.dot((y-(X.dot(beta))))\n",
    "        old_beta = beta - lr*grad\n",
    "#         print(cost)\n",
    "        beta = old_beta\n",
    "    return beta\n",
    "        \n",
    "        \n",
    "find_grad(X,y,beta)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 Describe the three types of gradient descent and discuss advantages and disadvantages of each. What are the main hyperparameters to tune in gradient descent?\n",
    "\n",
    "    Batch Gradient descent - batch gradient descent has to scan through the entire training set before taking a single\n",
    "                             step (updates on entire training set)\n",
    "                             \n",
    "                     -Faults: A lot of computing power, can be suseptible to local mins \n",
    "                     \n",
    "                     \n",
    "    Stochastic Gradient Descent - update the parameters according to the gradient of the error with respect to that\n",
    "                                  single training example only.(incrimental updates)\n",
    "                                  \n",
    "                     -Faluts: can be noisy, have trouble navigating saddlepoints \n",
    "                                \n",
    "    Mini batch Grad Descent - preforms an update on a batch of the training set\n",
    "       \n",
    "   Grad descent is susceptible to local miniumns and overfitting. \n",
    "   \n",
    "   This hyperparamter is alpha. Alpha is the learning rate\n",
    "   \n",
    "   Batch size is a hyperparamter\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4 Name some other optimization algorithms/ variants on gradient descent and give a brief overview of how they work.\n",
    "\n",
    "Momentum: \n",
    "    \n",
    "adaptive gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 You're trying to make model that will predict the female gold medal winner in the high jump in the next Olympics. You have results and data for 1000 high-jumpers in the past. You're using four features: jumper_height, jumper_weight, best_jump_height, and fastest_100m_time as features and your model performs ... just ok during cross-validation. Then it hits you: you also have maximum_squat_weight for all the jumpers, why don't you use that as a feature too? Using this additional feature (5 total now) during cross-validation, however, you get widely varying estimates of model performance. What do you think is going on?\n",
    "\n",
    "As a bonus, how many data points would you need with 5 features to have the same sample density as your model had with 4 features?\n",
    "\n",
    "    This is most likely do to overfitting your model. The high the complexity of your model the easier it is to overfit. Curse of dimensionality\n",
    "    \n",
    "    Becuase N^(1/d) = N2^(1/d2) we get 1000^(1/4)=N^(1/5) -----> N= 5606.4 (data points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6 Decision tree questions:\n",
    "\n",
    "Summary: Decision tree is an example of supervised, non-parametric learning. It goes through a series of binary splits based on certain featurs with the goal of minimizing predictive error. \n",
    "\n",
    "entropy = H(x) = -∑p<sub>i</sub>log<sub>2</sub>(p<sub>i</sub>)\n",
    "\n",
    "information gain = IG(s,c) = H(s) - ∑ abs(C<sub>i</sub>)/abs(s<sub></sub>)*H(C<sub>i</sub>)\n",
    "\n",
    "How to determine split: the descision tree considers all binary splits on a single feature. From that split the tree invisitages the <b> <i>information gain</i></b> and which ever split has the highest information gain the tree will split on. For <b> <i>regression</i></b> the splits are basically the same except the split is made by the highest information gain based on reduction of variance. Decision trees continue this process until the we are left with nodes which cannot be split. \n",
    "\n",
    "If decision tree is overfitting: You can use pruning techniques\n",
    "    1) you can stop splitting when samples get to a cetain size\n",
    "    2)stop splitting at a certain depth\n",
    "    3)stop splitting if there are a certain number of examples are the same class\n",
    "    4) when information gain gets to a certain threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7 What are the two things that make a random forest \"random\"? How do each of these things improve upon a standard Decision Tree?\n",
    "\n",
    "Random forrest makes individual trees more indepent from each other. In random forrest uses bootstrap to make several indepent trees. From there a similar tactic to decision trees are implements. Except on each split of the tree, a random n number of samples are choosen to split on. This value n is usually the square root of the number of features\n",
    "\n",
    "reduction in variance: Recall that given a set of n independent observations Z1,...,Zn, each with variance\n",
    "σ2, the variance of the mean Z-bar of the observations is given by σ2/n. In other words, averaging a set of observations reduces variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#8 you are a data scientist at a subscription company, and you decide to use a random forest model for predicting churn. The model works well, but your boss wants some insight into what factors contribute to customer churn. What can you tell him/her?\n",
    "\n",
    "You can tell your boss what features gave us the most insight into why customers churn. Feature Importance!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#9 Describe the hyperparameter C associated with SVMs, and how different values of C affect bias and variance of your model.\n",
    "\n",
    "The C represents the budget. This value modulates the misclassification boundries. \n",
    "    -Large C is hard boundries (accuracy more important)\n",
    "    -Small C is soft boundries (generlization more important)\n",
    "    \n",
    "Bias:\n",
    "    -A high-“bias” model makes many assumptions and prefers to solve problems a certain way.\n",
    "    -For complex data, high-bias models often underfit the data\n",
    "    \n",
    "variance:\n",
    "    -A high-“variance” model makes fewer assumptions and has more representational power.\n",
    "    -For simple data, high-variance models often overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#10  Describe the elastic net regularization parameter. Explain the two hyperparameters.\n",
    "\n",
    "Elatic net is a combination of Lasso (L1) and Ridge (L2) regularization.\n",
    "\n",
    "lamda and alpha are the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
