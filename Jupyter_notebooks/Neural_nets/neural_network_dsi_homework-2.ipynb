{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Homework\n",
    "=======================\n",
    "As we have only a day to talk about a very useful, and different, machine learning algorithm I'd like to make sure you come in Wednesday having already implemented the basic ideas behind neural networks.  To that end, you are assigned to complete this notebook **due Wednesday morning by 9 am**.\n",
    "\n",
    "To submit your homework, put your completed jupyter notebook and (possibly your completed script) in a folder named *firstname_lastname* (that's your first and last names, by the way), **zip it**, and Slack it to me.  I encourage you to treat this as an individual assignment, but feel free to discuss details with the instructors and your peers.  \n",
    "\n",
    "This homework relies heavily on three readings that are meant to help you understand and complete it:\n",
    "* [DL4J's Introduction to Deep Neural Networks Overview](https://deeplearning4j.org/neuralnet-overview)\n",
    "* [Carnegie Mellon's C.S. lecture: \"Neural Networks: A Simple Problem\"](https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/nn.pdf)\n",
    "* [i am trask's \"Neural Network in 11 lines of Python\"](https://iamtrask.github.io/2015/07/12/basic-python-network/)\n",
    "\n",
    "The objectives of this notebook are to:\n",
    "1. Introduce neural networks through a machine learning algorithm you already know: linear regression.\n",
    "2. Introduce the neuron as the computational unit of neural networks.\n",
    "3. During the training phase in a simple neural network, feed a signal forward through the network to make a prediction.  (this is **feed forward calculation**)\n",
    "4. After calculating the prediction error, use partial derivatives and the chain rule to calculate the changes to the weights required to decrease the training error. (this is **back propogation**) Then repeat feed-forward and back propogation until the residuals are minimized so that the weights are finally trained.  Once the weights are trained, the model can now predict on new data.\n",
    "5. Extrapolate feed-forward and back-propogation to a classification problem.\n",
    "6. Introduce multi-layer networks.\n",
    "7. In the optional `neural_network_hw.py` script, implement a multi-layer neural network to solve the [XOR problem](https://en.wikipedia.org/wiki/Perceptrons_%28book%29#The_XOR_affair), which contributed to an [A.I. Winter.](https://en.wikipedia.org/wiki/AI_winter)\n",
    "\n",
    "There are side objectives too:  practicing your [Markdown](http://daringfireball.net/projects/markdown/basics), embedding equations in a jupyter notebook with [MathJax](https://cdn.mathjax.org/mathjax/latest/test/examples.html), practicing [partial differential equations](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives/v/partial-derivatives-introduction), practicing the [chain rule](https://www.khanacademy.org/math/ap-calculus-ab/product-quotient-chain-rules-ab/chain-rule-ab/v/chain-rule-introduction), and of course ... [plotting](http://matplotlib.org/examples/)!\n",
    "\n",
    "Ok. Let's begin.  Obviously you can add cells in a section if you need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Raw data\n",
    "X = np.array([0.5, 2.5, 2.6, 3.5, 4.6, 4.8, 5, 6.2, 7.4, 8.1, 8.3, 9.2, 9.7])\n",
    "y = np.array([4.6, 7.4, 5.7, 11.6, 11.0, 13.0, 11.9, 15.0, 19.0, 16.9, 19.1, 21.9, 21.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEMCAYAAADAqxFbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFPlJREFUeJzt3X20ZXV93/H3Z7QGRlg1kSkQzcxgwIciKUkuxiggQTC2\nslJj2kYdo8SHWUpVFFgkOqbFRIxRK5KoKWO6FmhGMRGrYqFBG0dqUJKLWgQEEoVhIU9jxAjMgAG+\n/WOfK2dfmId779y9z8P7tdZZd/bDOed7D5f9Ob/92/v3S1UhSdKcFX0XIEkaLQaDJKnFYJAktRgM\nkqQWg0GS1GIwSJJaDAZpNyT5D0m8tltTwWDQyEhybpIaPO5PclOSP03yk33XthhJbkxyWt91SAtl\nMGjUfAE4EFgLvBo4AfhQnwVJ08Zg0Ki5r6puq6qbq+oS4C+A5w3vkOSUJFcmuSfJd5P8WZLHDW2/\nNcmLh5a/nOSuJI8eLB88aJU8cUdFJHl5ki1JtiX5HLD/vO0/m+QzSW4b1PG1JCcMbd8MrAHeM9cK\nGqx/fJKPJ7k5yfYkVyf57aV8YNKeZjBoZCV5EvB84J/nbXoQeBNwKPBS4BnAnwxt/xJwzOA1VgJH\nAPcBM4PtxwDfrqqbd/C+vwScC2wEDgcuBH5/3m77ABcDxwP/BrgA+FSSpw62vwi4efC8AwcPgL2A\nr9G0hA4FzgbOSfLcHX8SUrfiWEkaFUnOBV4G3As8iuYgCnBKVZ21k+c9H/gMsHdVPZjktcCbq+op\nSY6jCY3Lgeuq6g+T/Dlwb1W9egev9zFgVVUdP7Tuz4BXVVV2UsdXgc9V1TsGyzcCH6iq9+7i9z4f\nuHtH9Uhds8WgUXMpzbf0uVbARcAfD++Q5Ngknx+cjrkL+BTwGOCAwS6bgScnOZCmdfDFwbpjBtuf\nM1jekacBX5m3rrWc5LFJ3p3kmiR3JrmbpkWyeme/XJJHJdkwOBX2j4PnvWhXz5O6ZDBo1Gyrqn+o\nqm9W1RuBlcDvzW1Msgb4X8C3gP8I/CLwysHmxwBU1bXAbcCv0A6GZyd5GvBEdh4Mu+O9g/f/PZqg\nORz427kaduI04FTgPcBzB8/79G48T+rMo/suQNqFtwMXJ9lYVbfQfCt/DM2pogcAhjt9h3wJeMFg\n/81VtTXJ94DT2Un/wsC3gGfOWzd/+UjgI1V1waCGvYCfBa4f2udHNKfE5j/vwqr66OB5AZ4M/GAn\n9UidssWgkVZVm4FrgLcNVv09zd/tm5IclOQlNB3R820G/hPwD1W1dWjdy9h1a+GPgeOSvCXJIUle\nA/z6vH2uB349yS8kOQz4cx7qE5lzI3BUkick2W/oec9NcuSgo/oDwEG7qEfqlMGgcfDfgFclWVNV\nVwInA6fQBMaraU7PzLeZpkW8eRfrHqaqvgq8CngdcCVNH8AZ83Y7BbgD+L80Vyd9dfDvYf8F+Bng\n28BcOL2D5pTTxTT9KfcAm3ZWj9Q1r0qSJLXYYpAktRgMkqQWg0GS1GIwSJJaDAZJUovBIElqMRgk\nSS0GgySpxWCQJLUYDJKkFoNBktRiMEiSWgwGSVLLxEzUs99++9XatWv7LkOSenfFFVd8r6pWLfb5\nExMMa9euZXZ2tu8yJKl3SbYs5fmeSpIktRgMkqQWg0GS1GIwSJJaDAZJWmabNsHatbBiRfNz06a+\nK9q5ibkqSZJG0aZNsH49bNvWLG/Z0iwDrFvXX107Y4tBkpbRhg0PhcKcbdua9aPKYJCkZXTTTQtb\nPwoMBklaRqtXL2z9KDAYJGkZnXkmrFzZXrdyZbN+IbrswDYYJGkZrVsHGzfCmjWQND83blxYx/Nc\nB/aWLVD1UAf2coVDqmp5XrljMzMz5VhJkibR2rVNGMy3Zg3ceOPD1ye5oqpmFvt+thgkacR13YFt\nMEjSiOu6A9tgkKQRt6c6sHeXwSBJI25PdGAvhENiSNIYWLeuuyE0bDFIkloMBklSS2fBkOQtSf4u\nyQ+TbE1yYZKnz9snSc5IckuS7Uk2Jzm0qxolSd22GI4BPgQ8CzgWuB/4QpKfGtrndOBU4A3AEcAd\nwOeT7NthnZI01TrrfK6qXx1eTvJbwD8BzwYuTBLgTcC7quqCwT6voAmHlwLndFWrJE2zPvsY9h28\n/52D5YOAA4BL5naoqu3ApTStDElSB/oMhrOBbwBfGSwfMPh5+7z9bh/a1pJkfZLZJLNbt25dniol\nacr0EgxJ3gccCfxGVT2w2Nepqo1VNVNVM6tWrdpzBUrSFOs8GJKcBbwEOLaqvjO06bbBz/3nPWX/\noW2SpGXWaTAkOZuHQuHaeZtvoAmA44f23ws4CrissyIlacp1eR/DB4HfprnC6M4kBwwe+wBUMzHE\n+4HfSfKiwT0O5wJ3Ax/rqk5Jk63LmdDGVZcthpNorkT6P8CtQ4/ThvZ5N3AW8EFgFjgQeF5V3dVh\nnZI60vVBuuuZ0MaVM7hJ6sXcQXrbtofWrVy5vKOGLnQmtHHlDG6SxtKGDe1QgGZ5w4ble8+uZ0Ib\nVwaDpF70cZDueia0cWUwSOpFHwfprmdCG1cGg6Re9HGQ7nomtHHlDG6SejF3MN6woTl9tHp1EwrL\nfZDucia0cWUwSOqNB+nR5KkkSVKLwSBJajEYJEktBoMkqcVgkCS1GAySpBaDQZLUYjBIkloMBklS\ni8EgSWoxGCRNJKfwXDzHSpI0cebPDjc3hSc4NtPusMUgaeL0MTvcJDEYJE0cp/BcGoNB0sRxCs+l\nMRgkTRyn8Fwag0HSxHEKz6XxqiRJE8nZ4RbPFoMkqcVgkCS1GAySpBaDQZLUYjBIkloMBklSS6fB\nkOToJJ9N8t0kleTEedvPHawffny1yxoladp13WLYB7gKOBnYvoN9vgAcOPT4d92UJk0mh5/WQnV6\ng1tVXQRcBE3rYAe73VdVt3VWlDTBHH5aizGKfQxHJrkjyfVJPpzkX/VdkDSuHH5aizFqwfC/gZcD\nzwVOBZ4B/HWSn3iknZOsTzKbZHbr1q0dlimNB4ef1mKMVDBU1flV9dmq+mZVXQj8W+ApwAt2sP/G\nqpqpqplVq1Z1Wqs0Dhx+WosxUsEwX1XdAtwMHNJ3LdI4cvhpLcZIB0OSVcATgFv7rkUaRw4/rcXo\n9KqkJPsABw8WVwCrkxwOfH/wOAO4gCYI1gJ/CNwB/M8u65QmicNPa6G6bjHMAF8fPPYG3j749+8D\nDwCHAZ8BrgfOA64Dfrmq7uq4TkmaWl3fx7AZyE52+dWOSpEk7cBI9zFIkrpnMEiSWgwGSVKLwSBJ\najEYJEktBoMkqcVgkCS1GAzSAjjpjaZBpze4SePMSW80LWwxSLvJSW80LQwGaTc56Y2mhcEg7SYn\nvdG0MBik3eSkN5oWBoO0m5z0RtPCq5KkBXDSG00DWwySpBaDQdoDvPFNk8RTSdISeeObJo0tBvVu\n3L9te+ObJo0tBvVqEr5te+ObJs1utxiSfDrJCUlsZWiPmYRv2974pkmzkIP8PcAngJuTvDPJIctU\nk6bIJHzb9sY3TZrdDoaqWgccCPwBcBxwXZJLk7w8yd7LVaAm2yR82/bGN02aBZ0WqqofVtWfVtUz\ngMOAK4BzgFuTnJPkactRpCbXKH7bXkxn+Lp1cOON8OCDzU9DQeNsUf0FSX4a+PfACcD9wAXAzwBX\nJjltz5WnSTdq37bnOsO3bIGqhzrDx+1KKWkpUlW7t2PyL2jC4JXA8cDXgQ8DH6+quwf7/Brwkap6\n3PKUu2MzMzM1Ozvb9dtqwqxd24TBfGvWNC0BaRwkuaKqZhb7/IVcrnorEOBjwO9W1ZWPsM+lwJ2L\nLUbq2yR0hktLtZBgeDPwl1V17452qKofAActuSqpJ6tXP3KLYZw6w6WlWshVSR/dWShIk2AUO8Ol\nrnmzmjRk1DrDpT44JIY0j3MuaNp12mJIcnSSzyb5bpJKcuK87UlyRpJbkmxPsjnJoV3WKEnTrutT\nSfsAVwEnA9sfYfvpwKnAG4AjgDuAzyfZt7MKJWnKdRoMVXVRVb21qj4JPDi8LUmANwHvqqoLquoq\n4BXAvsBLu6xTkqbZKHU+HwQcAFwyt6KqttPcG/GsR3pCkvVJZpPMbt26tZsqJWnCjVIwHDD4efu8\n9bcPbWupqo1VNVNVM6tWrVrW4iRpWoxSMEiSRsAoBcNtg5/7z1u//9A2SdIyG6VguIEmAI6fW5Fk\nL+Ao4LK+ipKkadPpDW5J9gEOHiyuAFYnORz4flXdlOT9wFuTXAtcD7wNuJtm4D5JUge6vvN5Bvji\n0PLbB4/zgBOBdwN7Ax8EfhK4HHheVd3VbZmSNL06DYaq2kwzdPeOthdwxuAhSerBKPUxSJJGgMEg\nSWoxGCRJLQaDJKnFYJAktRgMkqQWg0GS1GIwSJJaDAZJUovBIElqMRgkSS0Gg9i0CdauhRUrmp+b\nNvVdkaQ+dT26qkbMpk2wfj1s29Ysb9nSLAOsW9dfXZL6Y4thym3Y8FAozNm2rVkvaToZDFPuppsW\ntl7S5DMYptzq1QtbL2nyGQxT7swzYeXK9rqVK5v1kqaTwTDl1q2DjRthzRpImp8bN9rxLE0zr0oS\n69YZBJIeYotBktRiMEiSWgwGSVKLwSBJajEYJEktBoMkqcVgUIsjrUryPgb9mCOtSgJbDBriSKuS\nwGDQEEdalQQGg4Y40qokGLFgSHJGkpr3uK3vuqaFI61KghELhoHrgAOHHof1W870cKRVSTCaVyXd\nX1W2EnriSKuSRrHF8KQktyS5Icn5SZ7Ud0GSNE1GLRguB04Eng+8BjgAuCzJ4x9p5yTrk8wmmd26\ndeui3tAbuiSpLVXVdw07lOSxwA3Au6rqfTvbd2ZmpmZnZxf0+vNv6IKms9Xz6pLGWZIrqmpmsc8f\ntRZDS1XdA1wNHLIcr+8NXZL0cCMdDEn2Ap4K3Locr+8NXZL0cCMVDEnem+Q5SQ5K8kvAJ4HHAuct\nx/t5Q5ckPdxIBQPwRODjNPcyfAq4D3hmVW1Zjjfzhi5JeriRuo+hql7c5fvNdTBv2NCcPlq9ugkF\nO54lTbORCoY+eEOXJLWN2qkkSVLPDAZJUovBIElqMRgkSS0GgySpxWCQJLUYDJKkFoNBktRiMEiS\nWgwGSVKLwSBJajEYJEktBoMkqcVgkCS1GAySpBaDQZLUYjBIkloMBklSi8EgSWoxGCRJLQaDJKnF\nYJAktRgMkqQWg0GS1GIwSJJaDAZJUovBIElqMRgkSS0GgySpZSSDIclJSW5Icm+SK5Ic1XdNkjQt\nRi4YkvwmcDbwTuDngcuAi5Os7rUwSZoSIxcMwCnAuVX14ar6VlW9AbgVeF3PdUnSVBipYEjyGOAX\ngUvmbboEeFb3FUnS9Hl03wXMsx/wKOD2eetvB46bv3OS9cD6weJ9Sa5a3vJG3n7A9/ouomd+Bn4G\n0/77AzxlKU8etWBYkKraCGwESDJbVTM9l9QrPwM/A/AzmPbfH5rPYCnPH6lTSTQp/wCw/7z1+wO3\ndV+OJE2fkQqGqvoRcAVw/LxNx9NcnSRJWmajeCrpfcBHk/wt8DfAa4GfBv77Lp63cbkLGwN+Bn4G\n4Gcw7b8/LPEzSFXtqUL2mCQnAacDBwJXAW+uqkv7rUqSpsNIBoMkqT8j1ccgSerf2AfDNI+rlOQt\nSf4uyQ+TbE1yYZKn911XnwafSSX5QN+1dCnJgUnOG/wd3JvkmiTP6buuriR5VJI/GDoW3JDkHUlG\nsR91j0hydJLPJvnu4G/+xHnbk+SMJLck2Z5kc5JDd+e1xzoYHFeJY4AP0dwVfixwP/CFJD/VZ1F9\nSfJMmhser+y7li4leRzNhRoBXgA8DXgDcEefdXXsd4D/DLwReCpwMnAS8JY+i1pm+9D0wZ4MbH+E\n7acDp9L8LRxB8/fw+ST77uqFx7qPIcnlwJVV9ZqhdX8PfLKqJvkP4hEl2Qf4J+CFVXVh3/V0Kcm/\nBL4GvBr4r8BVVfX6fqvqRpJ3As+pqmf3XUtfknwO+MeqesXQuvOAx1fVCf1V1o0kdwOvr6pzB8sB\nbgE+UFVnDtbtTRMOp1XVOTt7vbFtMTiu0iPal+a/6Z19F9KDjTRfCL7YdyE9eCFweZJPJLkjyTeS\nvH5wcJgWXwZ+JclTAZL8a5pW9EW9VtWfg4ADGDo+VtV24FJ24/g4zuffFjSu0pQ4G/gG8JW+C+lS\nktcABwMv67uWnjyJ5rTJWcC7gMOBPxlsm5a+lj+i+WJ0TZIHaI5tZ1bVh/otqzcHDH4+0vHxCbt6\n8jgHg4YkeR9wJHBkVT3Qdz1dSfIUmj6mI6vqn/uupycrgNmh06dfT3IIzTn3aQmG3wReDrwUuJom\nHM9OckNV/Y9eKxtDY3sqCcdV+rEkZwEvAY6tqu/0XU/Hfpmm9Xh1kvuT3A88BzhpsPwT/ZbXiVuB\na+at+xYwLRdhALwHeG9VnV9V36yqj9KMojB1fY0Dc8fARR0fxzYYHFepkeRsHgqFa/uupwefBg6j\n+YY495gFzh/8+0f9ldaZv+Hhwyw/GdjSQy19WUnzRXHYA4zxMW6JbqAJgB8fH5PsBRzFbhwfx/1U\n0mLHVZoIST4I/BZN5+OdSebOK95dVXf3V1l3quoHwA+G1yW5B/h+VU3L/BxnAZcl2QB8gubS7TcC\nb+21qm5dCPxukhtoTiX9PM1skB/ptaplNLgK8eDB4gpgdZLDaf72b0ryfuCtSa4FrgfeBtwNfGyX\nL15VY/2g6XS7EbiPpgVxdN81dfi71w4eZ/RdW8+fy2aay/R6r6XD3/kFwP8D7h0cBN7I4HL0aXjQ\ndDy/n6aVtB34Dk3f015917aMv/MxO/j//9zB9gBn0JxqvBf4EvD03Xntsb6PQZK0503r+TdJ0g4Y\nDJKkFoNBktRiMEiSWgwGSVKLwSBJajEYJEktBoO0BElWJLk0yYXz1q9Mcl2SqbgLX5PFYJCWoKoe\nBE6kmQvglUOb/ohmWPhT+6hLWgrvfJb2gCSvpQmDn6MZv+avgGOq6su9FiYtgsEg7SFJ/grYG1gL\nnF9Vp/dbkbQ4BoO0hyQ5CPj24PH0qrqv55KkRbGPQdpzXkkzsucTaebclcaSLQZpD0hyBM0EKL8G\nvI5mpqxn1RRNs6rJYYtBWqLBzFgfoRkH/2JgPU0HtH0MGku2GKQlGsy5/ULg56rqrsG6FwPnAb9Q\nVVf3WZ+0UAaDtARJjgb+GjiuqjbP2/aXNH0Nz6yq+3soT1oUg0GS1GIfgySpxWCQJLUYDJKkFoNB\nktRiMEiSWgwGSVKLwSBJajEYJEktBoMkqeX/AznO/uRTSDUvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e3a90f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(X, y, 'bo', label = 'data')\n",
    "ax.set_xlabel('X', fontsize = 14) \n",
    "ax.set_ylabel('y', fontsize = 14)\n",
    "ax.set_xlim([0, 10])\n",
    "ax.set_ylim([0, 23])\n",
    "fig.suptitle('Raw data', fontsize = 14)\n",
    "fig.tight_layout(pad=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict **y** as function of **X**.  Let's not use regularization, but rather solve this linear regression using the closed form solution we know: ![least_squares](images/least_squares.png)  \n",
    "However, we can agree that this equation is pixelated and hard to read.  Redo this equation using Markdown and MathJax in the cell below.  Some excellent examples of what MathJax can do can be found [here](https://cdn.mathjax.org/mathjax/latest/test/sample.html) and you can see the MathJax code by viewing the source code in your browser (type in  `view-source:https://cdn.mathjax.org/mathjax/latest/test/sample.html` to the browser address window if you can't figure that out.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Type the least squares closed form algebraic equation shown above.  Some starter code is given below (double click on this cell.)\n",
    "\n",
    "$$\n",
    "\\theta = (X^T  \\vec{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Use numpy to solve for the $\\theta$ coefficients of your linear least squares problem.  Don't forget to add a column of 1s to X so that you get an intercept.  But instead of using $\\theta$ use $\\boldsymbol{w}$ (for weight).  What is the slope ($\\boldsymbol{w_1}$)?  What is the intercept ($\\boldsymbol{w_0})$?  Plot the regression versus the data using a red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot data and regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great.  So what's this have to do with neural networks?  We are going to use the simplest computation unit of a neural network, a linear neuron (a.k.a. node, neurode, unit), to calcuate the weights determined from the linear regression, and in the process demonstrate **feed forward** calculation and **back propogation**.\n",
    "\n",
    "Consider the following neuron ![simple_neuron](images/simple_neuron.png).\n",
    "\n",
    "Data ($\\boldsymbol{X}$) is flowing into this neuron through its *synapses* (otherwise known as weights).  From there the weighted data is summed: $w_0x_0$ + $w_1x_1$, (equivalently $\\boldsymbol{w} \\cdot \\boldsymbol{X}$, or $\\boldsymbol{w}^{T}\\boldsymbol{X}$).  Then this weighted sum is evaluated through an activation function, $\\boldsymbol{\\phi}$, to get prediction $y_p$.  For now, assume that the activation function only multiplies the sum by 1!  This makes this neuron a linear neuron and the calculation of $y_p$ very easy.\n",
    "\n",
    "To start forward calculations, assumptions need to be made about the starting values of the weights.  In modern neural nets, determining the proper initializations of the weights is an active area of research. Assume for now they were initialized according to a uniform distributions between -1 and 1, with $w_0$ = -0.5, and $w_1$ = 0.25.\n",
    "\n",
    "Note that $x_0$ and $x_1$ correspond to the two values associated with each row of $\\boldsymbol{X}$, where $\\boldsymbol{X}$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_lsq = np.vstack([np.ones(len(X)), X]).T\n",
    "print(\"X (with a constant) is:\")\n",
    "print(\"   x0   x1\")\n",
    "print(X_lsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:  With the assumed weights $w_0$ = -0.5, $w_1$ = 0.25 and the first row of $\\boldsymbol{X}$, what value of $y_p$ results from feed-forward calculation? (Don't overthink this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously a $y_p$ value of -0.375 is not near the first $y_{train}$ value of 4.6.  The weights need to be modified before they predict on the next value of y.  Corrections to the weights are applied in a process called **back propogation**.  In this process, partial derivates, the chain rule, and gradient descent are used to predict corrections for each of the weights before feed-forward calculation on the next training sample.\n",
    "\n",
    "Recall that the loss, or prediction error, for one prediction during training is\n",
    "$$\n",
    "E = \\frac{1}{2}(y_{train} - y_p)^2\n",
    "$$\n",
    "\n",
    "and the goal is to calculate $\\Delta w_0$ and $\\Delta w_1$, and where $y_p$ = $\\phi(w^{T}X)$.  Using **gradient descent** the change of weights can be approximated as: \n",
    "\n",
    "$$\n",
    "\\Delta w = - \\alpha \\frac{\\partial E}{\\partial w}\n",
    "$$\n",
    "\n",
    "and from the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial \\phi} \\frac{\\partial \\phi}{\\partial w}\n",
    "$$\n",
    "\n",
    "where $\\phi$ is the activation function (multiplying by 1 in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:  Calculate by hand, and then express in MathJax in this cell, the equations that solve for $\\Delta w_0$ and $\\Delta w_1$.\n",
    "\n",
    "$$\n",
    "\\Delta w_0 =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta w_1 =\n",
    "$$\n",
    "\n",
    "Hint: first express the loss, $E$, as a function of $w$, then find the partial derivative of $E$ with respect to $w$.  Substitute in $x_0$ to for $x$ find $w_0$, and $x_1$ for $x$ to find $w_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5:  Use your $\\Delta w$ equations to back propogate changes to your weights for each row in your training set.  Just pass through your data set once.  Are your $w_0$ and $w_1$ values similar to those predicted by the linear regression?  What if you continue to cycle through your data set?  Are your weights converging to the linear regression values?  You will need to write a simulation to answer these questions.  For me, convergence was highly dependent on learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problems a neuron with a linear activation function doesn't work well.  Additionally, neurons with solely linear behavior tend to be limited in their ability to learn non-linear features.  Use of the logistic function as an activation function (and other activation functions we'll learn about in class) addresses these liabilities.  Consider the following data, targets and neuron architecture.\n",
    "\n",
    "![neuron_3inputs](images/neuron_3inputs.png)\n",
    "\n",
    "This is the same neuron and data analyzed in Part 1 of [A Neural Network in 11 lines of Python](https://iamtrask.github.io/2015/07/12/basic-python-network/).  Note that there are four rows of data, and four predictions that are classes 1 and 0.  Additionally, the column $x_0$ is the bias column (the column where all values are 1 so that weight $w_0$ can represent the intercept).\n",
    "\n",
    "The activation function $\\phi$ is the standard logistic function, often referred to as the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function).  It has the property of taking continuously valued input and mapping it to a 0-1 range, just as it did in logistic regression.\n",
    "\n",
    "In this neuron:\n",
    "\n",
    "$$\n",
    "y_p = \\phi(w^{T}X)\n",
    "$$\n",
    "$$\n",
    "\\phi(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:  Say that we are training on the 2nd row of data ($x_0$, $x_1$, $x_2$ = 1, 1, 1 and $y_t$ = 1) and that our weights were $w_0$, $w_1$, $w_2$ = -0.5, 2, 1.  What would $y_p$ be in this feed forward calculation?  Do not threshold $y_p$ to 0 or 1, maintain its fractional value as calculated by the sigmoid function.  And what is the error in the prediction of this data point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7:  To correct the error the weights must be updated using back propogation, just as in the linear case.  However, now the activation function has a more complicated derivative than the linear case.  What are the equations now to update the weights?\n",
    "\n",
    "$$\n",
    "\\Delta w_0 =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta w_1 =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta w_2 =\n",
    "$$\n",
    "\n",
    "Hint:  here's a start to the derivations:\n",
    "\n",
    "To simplify derivation and not deal with vector quantities, will only look at one weight $w$ and its corresponding x value $x$.\n",
    "\n",
    "$$\n",
    "\\Delta w = - \\alpha \\frac{\\partial E}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial \\phi} \\frac{\\partial \\phi}{\\partial w}\n",
    "$$\n",
    "\n",
    "Let's work on the first quantity, $\\frac{\\partial E}{\\partial \\phi}$\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2}(y_{train} - y_p)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_p = \\phi(wx)\n",
    "$$\n",
    "\n",
    "...  \n",
    "...\n",
    "\n",
    "Now the second quantity, $\\frac{\\partial \\phi}{\\partial w}$\n",
    "\n",
    "$$\n",
    "\\phi(wx) = \\frac{1}{1 + e^{-wx}}\n",
    "$$\n",
    "\n",
    "[Derivative of sigmoid](http://www.ai.mit.edu/courses/6.892/lecture8-html/sld015.htm), and substituting $q$ for $x$ so as to keep it different from the $x$ in our derivation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\phi}{\\partial q} = q \\cdot (1-q)\n",
    "$$\n",
    "\n",
    "setting $q = wx$, and then using the chain rule:\n",
    "\n",
    "...  \n",
    "...\n",
    "\n",
    "Now putting it all together:\n",
    "...  \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8:  Implement and try to understand the code of the \"2-Layer Network\" presented in the \"Neural Network in 11 lines of Python\" reference.   You should be able to converge on weights that perfectly fit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this neuron can learn another logical function, the exclusive or (XOR).  Subsitute this X and y array into your 2-layer network and see if it converges.\n",
    "\n",
    "![XOR_data](images/XOR_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9:  Unless you did something, it shouldn't converge.  In fact, it's impossible that it converged.  Why is that?  (May have to do some reading on this one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
