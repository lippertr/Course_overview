{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sympy can be used to check your calculus\n",
    "\n",
    "A simple derivative example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3*x**2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate symbol first\n",
    "x = sympy.symbols('x')\n",
    "# take derivative\n",
    "sympy.diff(x**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x**3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indefinite integral\n",
    "sympy.integrate(3*x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definite integral\n",
    "sympy.integrate(3*x**2, (x, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = sympy.Matrix(sympy.symbols('a b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Matrix([\n",
       " [a],\n",
       " [b]]), Matrix([[a, b]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, m.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2*a + 2*b, 2*a + 2*b]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sympy.diff(sum(m*m.T), i) for i in m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[a**2,  a*b],\n",
       "[ a*b, b**2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for reference...this is what this looks like...\n",
    "m*m.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decent and Newton's method\n",
    "\n",
    "Materials in this notebook was are adopted from Isaac Laughlin's lecture materials.\n",
    "Here is some starter code to get you going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newtons_method(f_prime, f_double_prime, initial_guess,\n",
    "                   threshold, max_iter):\n",
    "    x = float(initial_guess)\n",
    "    iterations = 0\n",
    "    x_history = [x]\n",
    "    while f_prime(x) > threshold and iterations < max_iter:\n",
    "        x = x - f_prime(x)/f_double_prime(x)\n",
    "        x_history.append(x)\n",
    "        iterations += 1\n",
    "    print('newtons method took %s iterations'%iterations)\n",
    "    return x_history\n",
    "\n",
    "def gradient_descent(f, f_prime, initial_guess, threshold, \n",
    "                     max_iter, learning_rate):\n",
    "    x = float(initial_guess)\n",
    "    iterations = 0\n",
    "    x_history = [x]\n",
    "    while f_prime(x) > threshold and iterations < max_iter:\n",
    "        x = x - learning_rate*f_prime(x)\n",
    "        x_history.append(x)\n",
    "        iterations += 1\n",
    "    print('gradient descent took %s iterations'%iterations)\n",
    "    return x_history\n",
    "\n",
    "def f(x): return x**4+x**2+10\n",
    "def f_prime(x): return 4*x**2 + 2*x\n",
    "def f_double_prime(x): return 8*x + 2\n",
    "\n",
    "initial_guess = 10\n",
    "threshold = .001\n",
    "max_iter = 1000\n",
    "learning_rate = .01\n",
    "\n",
    "newton_x_history = newtons_method(f_prime, \n",
    "                                  f_double_prime,\n",
    "                                  initial_guess, \n",
    "                                  threshold,\n",
    "                                  max_iter)\n",
    "\n",
    "gradient_descent_x_history = gradient_descent(f, f_prime,\n",
    "                                              initial_guess,\n",
    "                                              threshold, \n",
    "                                              max_iter, \n",
    "                                              learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
